import os
import sys
import uuid
from typing import Any, List, Iterable
from dotenv import load_dotenv

# Ensure you have the latest required packages
# pip install -U langchain-azure-ai mem0 langchain-openai azure-cosmos azure-identity

from langchain_azure_ai.vectorstores import AzureCosmosDBNoSqlVectorSearch
from mem0 import Memory
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from azure.cosmos import CosmosClient
from azure.identity import DefaultAzureCredential
from langchain_core.documents import Document

# --- Load Environment Variables ---
load_dotenv()

# --- Azure & Cosmos DB Configuration ---
azure_openai_api_key = os.getenv("AZURE_OPENAI_API_KEY")
azure_openai_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
azure_openai_api_version = os.getenv("AZURE_OPENAI_API_VERSION")
azure_openai_deployment_name = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")
azure_openai_embedding_deployment_name = os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME")
cosmosdb_endpoint = os.getenv("AZURE_COSMOSDB_ENDPOINT")
cosmosdb_database_name = os.getenv("AZURE_COSMOSDB_DATABASE_NAME")
cosmosdb_container_name = "mem0"


# --- Wrapper Class to Bridge Library Incompatibilities ---

class CosmosDBMem0Wrapper(AzureCosmosDBNoSqlVectorSearch):
    """
    A wrapper for AzureCosmosDBNoSqlVectorSearch to make it fully
    compatible with mem0. Handles proper document formatting and ID management.
    """
    
    def add_texts(
        self,
        texts: Iterable[str],
        metadatas: List[dict] | None = None,
        **kwargs: Any,
    ) -> List[str]:
        """Override add_texts to ensure proper ID handling."""
        if not metadatas:
            metadatas = [{} for _ in texts]
            
        # Ensure each metadata has an ID
        for metadata in metadatas:
            if 'id' not in metadata:
                metadata['id'] = str(uuid.uuid4())
                
        # Call parent's add_texts
        return super().add_texts(texts, metadatas, **kwargs)

    def similarity_search_by_vector(
        self, embedding: List[float], k: int = 4, **kwargs: Any
    ) -> List[Document]:
        """Perform similarity search using the vector."""
        try:
            results = self.similarity_search_with_score(
                query="",  # Empty string instead of None
                k=k,
                vector=embedding,
                **kwargs
            )
            
            # Format documents with proper metadata structure for Mem0
            formatted_docs = []
            for doc, score in results:
                # Start with original metadata or empty dict
                metadata = doc.metadata.copy() if doc.metadata else {}
                
                # Ensure required fields
                metadata['data'] = doc.page_content
                if 'id' not in metadata:
                    # Try to get ID from various possible locations
                    metadata['id'] = (
                        metadata.get('_id') or 
                        metadata.get('id') or 
                        metadata.get('document_id') or 
                        str(uuid.uuid4())
                    )
                
                # Create formatted document
                formatted_doc = Document(
                    page_content=doc.page_content,
                    metadata=metadata
                )
                formatted_docs.append(formatted_doc)
            
            return formatted_docs
        except Exception as e:
            print(f"Error in similarity_search_by_vector: {e}")
            # Return empty list on error to allow graceful degradation
            return []

# --- Initialize Clients ---
print("Initializing clients...")

embedder = AzureOpenAIEmbeddings(
    azure_deployment=azure_openai_embedding_deployment_name,
    openai_api_version=azure_openai_api_version,
    azure_endpoint=azure_openai_endpoint,
    api_key=azure_openai_api_key
)
EMBEDDING_DIMENSIONS = 1536

llm = AzureChatOpenAI(
    azure_deployment=azure_openai_deployment_name,
    api_version=azure_openai_api_version,
    azure_endpoint=azure_openai_endpoint,
    api_key=azure_openai_api_key,
)

credential = DefaultAzureCredential()
cosmos_client = CosmosClient(url=cosmosdb_endpoint, credential=credential)

vector_embedding_policy = {
    "vectorEmbeddings": [
        {"path": "/vectorContent", "dataType": "float32", "dimensions": EMBEDDING_DIMENSIONS, "distanceFunction": "cosine"},
    ]
}
indexing_policy = {
    "includedPaths": [
        {"path": "/*"},
        {"path": "/metadata/*"}  # Ensure metadata fields are indexed
    ],
    "excludedPaths": [{"path": '/"_etag"/?'}],
    "vectorIndexes": [
        {"path": "/vectorContent", "type": "quantizedFlat"},
    ],
}

# --- Instantiate our Wrapper Class with the Correct Constructor Arguments ---
print("Initializing Azure Cosmos DB for NoSQL Vector Store...")
vector_store = CosmosDBMem0Wrapper( # <-- Use the wrapper here
    cosmos_client=cosmos_client,
    database_name=cosmosdb_database_name,
    container_name=cosmosdb_container_name,
    embedding=embedder,
    vector_embedding_policy=vector_embedding_policy,
    indexing_policy=indexing_policy,
    cosmos_container_properties={"partition_key": {"paths": ["/id"], "kind": "Hash"}},
    cosmos_database_properties={},
    # *** THE DEFINITIVE FIX IS HERE ***
    # The `text_key` and `embedding_key` must be inside the `vector_search_fields` dictionary.
    # The `text_key` is set to "text" to match the field name mem0 uses internally.
    vector_search_fields={
        "text_field": "text",
        "embedding_field": "vectorContent",
        "metadata_field": "metadata"
    }
)
print("Vector store initialized successfully.")

# --- Configure Mem0 ---
config = {
    "vector_store": {
        "provider": "langchain",
        "config": { "client": vector_store }
    },
    "llm": {
        "provider": "langchain",
        "config": { "model": llm, }
    },
    "embedder": {
        "provider": "langchain",
        "config": { "model": embedder, }
    },
}

# --- Create Memory Instance and Use It ---
print("\nInitializing Mem0 from config...")
m = Memory.from_config(config)
print("Mem0 initialized.")

messages = [
    {"role": "user", "content": "My favorite movies are 'The Matrix' and 'Inception'."},
    {"role": "assistant", "content": "Great choices!"},
    {"role": "user", "content": "I also love 'Blade Runner 2049'."},
    {"role": "assistant", "content": "A modern classic!"}
]

# This will now work correctly.
print("\nAdding memories...")
result = m.add(messages, user_id="alice", metadata={"category": "movies"})
print("Added memories:", result)

# This will now find and return the stored memories.
print("\nSearching for memories...")
search_results = m.search("What movies does the user like?", user_id="alice")
print("Search results:", search_results)
